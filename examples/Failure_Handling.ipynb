{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling failed evaluation of few of the objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are simulating failures at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c23a4d25654b0c98935b4b48dad76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation failed\n",
      "Evaluation failed\n",
      "Evaluation failed\n",
      "Evaluation failed\n",
      "\n",
      "best parameters: {'x': -0.016781443194481938, 'y': -0.022917881560944764}\n",
      "best objective: -0.0008068461309311162\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The objective function return can be a list of successful parameters evaluated via a list of\n",
    "evaluations and respective hyperparameters. In case none is successful, the empty list can be returned.\n",
    "'''\n",
    "\n",
    "from mango.tuner import Tuner\n",
    "from scipy.stats import uniform\n",
    "import random\n",
    "\n",
    "param_dict = {\n",
    "    'x': uniform(-5, 10),\n",
    "    'y': uniform(-5, 10),\n",
    "}\n",
    "\n",
    "\n",
    "# Randomly fail the evaluatioon\n",
    "\n",
    "def objfunc(args_list):\n",
    "    hyper_evaluated = []\n",
    "    objective_evaluated = []\n",
    "    for hyper_par in args_list:\n",
    "        \n",
    "        To_do = random.random()\n",
    "        \n",
    "        if To_do>0.3:\n",
    "        \n",
    "            x = hyper_par['x']\n",
    "            y = hyper_par['y']\n",
    "            objective = -(x**2 + y**2)\n",
    "            objective_evaluated.append(objective)\n",
    "            hyper_evaluated.append(hyper_par)\n",
    "        \n",
    "        # This is failure, do nothing\n",
    "        else:\n",
    "            print(\"Evaluation failed\")\n",
    "            continue\n",
    "            \n",
    "    return hyper_evaluated, objective_evaluated\n",
    "\n",
    "tuner = Tuner(param_dict, objfunc)\n",
    "results = tuner.maximize()\n",
    "\n",
    "print('best parameters:',results['best_params'])\n",
    "print('best objective:',results['best_objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
