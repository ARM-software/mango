{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "from math import atan2, pi, sqrt, cos, sin, floor\n",
    "from data_utils import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess2 = tf.compat.v1.Session(config=config)\n",
    "set_session(sess2)  \n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mango.tuner import Tuner\n",
    "from scipy.stats import uniform\n",
    "from keras_flops import get_flops\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import quaternion\n",
    "import math\n",
    "from hardware_utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d34697",
   "metadata": {},
   "source": [
    "## Import Training, Validation and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100\n",
    "window_size = 200\n",
    "stride = 10\n",
    "f = '/home/nesl/swapnil/TinyOdom/Human/datasets/oxiod/' #dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set\n",
    "X, Y_disp, Y_head, Y_pos, x0_list, y0_list, size_of_each, x_vel, y_vel, head_s, head_c, X_orig = import_oxiod_dataset(type_flag = 2, \n",
    "                        useMagnetometer = True, useStepCounter = True, AugmentationCopies = 0,\n",
    "                         dataset_folder = f,\n",
    "                         sub_folders = ['handbag/','handheld/','pocket/','running/','slow_walking/','trolley/'],\n",
    "                         sampling_rate = sampling_rate, \n",
    "                         window_size = window_size, stride = stride, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Set\n",
    "X_val, Y_disp_val, Y_head_val, Y_pos_val, x0_list_val, y0_list_val, size_of_each_val, x_vel_val, y_vel_val, head_s_val, head_c_val, X_orig_val = import_oxiod_dataset(type_flag = 3, \n",
    "                        useMagnetometer = True, useStepCounter = True, AugmentationCopies = 0,\n",
    "                         dataset_folder = f,\n",
    "                         sub_folders = ['handbag/','handheld/','pocket/','running/','slow_walking/','trolley/'],\n",
    "                         sampling_rate = sampling_rate, \n",
    "                         window_size = window_size, stride = stride, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set\n",
    "X_test, Y_disp_test, Y_head_test, Y_pos_test, x0_list_test, y0_list_test, size_of_each_test, x_vel_test, y_vel_test, head_s_test, head_c_test, X_orig_test = import_oxiod_dataset(type_flag = 4, \n",
    "                        useMagnetometer = True, useStepCounter = True, AugmentationCopies = 0,\n",
    "                         dataset_folder = f,\n",
    "                         sub_folders = ['handbag/','handheld/','pocket/','running/','slow_walking/','trolley/'],\n",
    "                         sampling_rate = sampling_rate, \n",
    "                         window_size = window_size, stride = stride, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e35cf",
   "metadata": {},
   "source": [
    "## Training and NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"NUCLEO_F746ZG\" #hardware name\n",
    "model_name = 'TD_Oxiod_'+device+'.hdf5'\n",
    "dirpath=\"/home/nesl/Mbed Programs/tinyodom_tcn/\" #hardware program directory\n",
    "HIL = True #use real hardware or proxy?\n",
    "quantization = False #use quantization or not?\n",
    "model_epochs = 900 #epochs to train each model for\n",
    "NAS_epochs = 50 #epochs for hyperparameter tuning\n",
    "output_name = 'g_model.tflite'\n",
    "log_file_name = 'log_NAS_Oxiod_'+device+'.csv'\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "row_write = ['score', 'rmse_vel_x','rmse_vel_y','RAM','Flash','Flops','Latency',\n",
    "                 'nb_filters','kernel_size','dilations','dropout_rate','use_skip_connections','norm_flag']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f76602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(epochs=500,nb_filters=32,kernel_size=7,dilations=[1, 2, 4, 8, 16, 32, 64, 128],dropout_rate=0,\n",
    "                 use_skip_connections=False,norm_flag=0):\n",
    "    \n",
    "    inval = 0\n",
    "    rmse_vel_x = 'inf'\n",
    "    rmse_vel_y = 'inf'\n",
    "    batch_size, timesteps, input_dim = 256, window_size, X.shape[2]\n",
    "    i = Input(shape=(timesteps, input_dim))\n",
    "    \n",
    "    if(norm_flag==1):\n",
    "        m = TCN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,dropout_rate=dropout_rate,\n",
    "                use_skip_connections=use_skip_connections,use_batch_norm=True)(i)\n",
    "    else:\n",
    "        m = TCN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,dropout_rate=dropout_rate,\n",
    "                use_skip_connections=use_skip_connections)(i)        \n",
    "        \n",
    "    m = tf.reshape(m, [-1, nb_filters, 1])\n",
    "\n",
    "    m = MaxPooling1D(pool_size=(2))(m)\n",
    "    m = Flatten()(m)\n",
    "    m = Dense(32, activation='linear', name='pre')(m)\n",
    "    output1 = Dense(1, activation='linear', name='velx')(m)\n",
    "    output2 = Dense(1, activation='linear', name='vely')(m)\n",
    "    model = Model(inputs=[i], outputs=[output1, output2])\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss={'velx': 'mse','vely':'mse'},optimizer=opt)  \n",
    "    Flops = get_flops(model, batch_size=1)\n",
    "    convert_to_tflite_model(model=model,training_data=X,quantization=quantization,output_name=output_name) \n",
    "    maxRAM, maxFlash = return_hardware_specs(device)\n",
    "    \n",
    "    if(HIL==True):\n",
    "        convert_to_cpp_model(dirpath)\n",
    "        RAM, Flash, Latency, idealArenaSize, errorCode = HIL_controller(dirpath=dirpath,\n",
    "                                                                       chosen_device=device,\n",
    "                                                                       window_size=window_size, \n",
    "                                                                    number_of_channels = input_dim,\n",
    "                                                                   quantization=quantization)     \n",
    "        score = -5.0\n",
    "        if(Flash==-1):\n",
    "            row_write = [score, rmse_vel_x,rmse_vel_y,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,dropout_rate,use_skip_connections,norm_flag]\n",
    "            print('Design choice:',row_write)\n",
    "            with open(log_file_name, 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(row_write)\n",
    "            return score\n",
    "        \n",
    "        elif(Flash!=-1):\n",
    "            checkpoint = ModelCheckpoint(model_name, monitor='loss', verbose=1, save_best_only=True)\n",
    "            model.fit(x=X, y=[x_vel, y_vel],epochs=epochs, shuffle=True,callbacks=[checkpoint],batch_size=batch_size)     \n",
    "            model = load_model(model_name,custom_objects={'TCN': TCN})\n",
    "            y_pred = model.predict(X_val)\n",
    "            rmse_vel_x = mean_squared_error(x_vel_val, y_pred[0], squared=False)\n",
    "            rmse_vel_y = mean_squared_error(y_vel_val, y_pred[1], squared=False)\n",
    "            model_acc = -(rmse_vel_x+rmse_vel_y) \n",
    "            resource_usage = (RAM/maxRAM) + (Flash/maxFlash) \n",
    "            score = model_acc + 0.01*resource_usage - 0.05*Latency  #weigh each component as you like\n",
    "                \n",
    "            row_write = [score, rmse_vel_x,rmse_vel_y,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,dropout_rate,use_skip_connections,norm_flag]\n",
    "            print('Design choice:',row_write)\n",
    "            with open(log_file_name, 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(row_write)\n",
    "            \n",
    "    else:\n",
    "        score = -5.0\n",
    "        Flash = os.path.getsize(output_name)\n",
    "        RAM = get_model_memory_usage(batch_size=1,model=model)\n",
    "        Latency=-1\n",
    "        max_flops = (30e6)\n",
    "    \n",
    "        if(RAM < maxRAM and Flash<maxFlash):\n",
    "            checkpoint = ModelCheckpoint(model_name, monitor='loss', verbose=1, save_best_only=True)\n",
    "            model.fit(x=X, y=[x_vel, y_vel],epochs=epochs, shuffle=True,callbacks=[checkpoint],batch_size=batch_size)     \n",
    "            model = load_model(model_name,custom_objects={'TCN': TCN})\n",
    "            y_pred = model.predict(X_val)\n",
    "            rmse_vel_x = mean_squared_error(x_vel_val, y_pred[0], squared=False)\n",
    "            rmse_vel_y = mean_squared_error(y_vel_val, y_pred[1], squared=False)\n",
    "            model_acc = -(rmse_vel_x+rmse_vel_y) \n",
    "            resource_usage = (RAM/maxRAM) + (Flash/maxFlash)\n",
    "            score = model_acc + 0.01*resource_usage - 0.05*(Flops/max_flops)  #weigh each component as you like\n",
    "            \n",
    "        row_write = [score, rmse_vel_x,rmse_vel_y,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,dropout_rate,use_skip_connections,norm_flag]\n",
    "        print('Design choice:',row_write)\n",
    "        with open(log_file_name, 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(row_write)  \n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_res(data, file_name):\n",
    "    pickle.dump( data, open( file_name, \"wb\" ) )\n",
    "    \n",
    "min_layer = 3\n",
    "max_layer = 8\n",
    "a_list = [1,2,4,8,16,32,64,128,256]\n",
    "all_combinations = []\n",
    "dil_list = []\n",
    "for r in range(len(a_list) + 1):\n",
    "    combinations_object = itertools.combinations(a_list, r)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list\n",
    "all_combinations = all_combinations[1:]\n",
    "for item in all_combinations:\n",
    "    if(len(item) >= min_layer and len(item) <= max_layer):\n",
    "        dil_list.append(list(item))\n",
    "        \n",
    "param_dict = {\n",
    "    'nb_filters': range(2,64),\n",
    "    'kernel_size': range(2,16),\n",
    "    'dropout_rate': np.arange(0.0,0.5,0.1),\n",
    "    'use_skip_connections': [True, False],\n",
    "    'norm_flag': np.arange(0,1),\n",
    "    'dil_list': dil_list\n",
    "}\n",
    "\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        nb_filters = hyper_par['nb_filters']\n",
    "        kernel_size = hyper_par['kernel_size']\n",
    "        dropout_rate = hyper_par['dropout_rate']\n",
    "        use_skip_connections = hyper_par['use_skip_connections']\n",
    "        norm_flag=hyper_par['norm_flag']\n",
    "        dil_list = hyper_par['dil_list']\n",
    "            \n",
    "        objective = objective_NN(epochs=model_epochs,nb_filters=nb_filters,kernel_size=kernel_size,\n",
    "                                 dilations=dil_list,\n",
    "                                 dropout_rate=dropout_rate,use_skip_connections=use_skip_connections,\n",
    "                                 norm_flag=norm_flag)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated\n",
    "\n",
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1\n",
    "conf_Dict['num_iteration'] = NAS_epochs\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44af5d3",
   "metadata": {},
   "source": [
    "## Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce20e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = results['best_params']['nb_filters']\n",
    "kernel_size = results['best_params']['kernel_size']\n",
    "dilations = results['best_params']['dilations']\n",
    "dropout_rate = results['best_params']['dropout_rate']\n",
    "use_skip_connections = results['best_params']['use_skip_connections']\n",
    "norm_flag = results['best_params']['norm_flag']\n",
    "\n",
    "batch_size, timesteps, input_dim = 256, window_size, X.shape[2]\n",
    "i = Input(shape=(timesteps, input_dim))\n",
    "if(norm_flag==1):\n",
    "    m = TCN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,dropout_rate=dropout_rate,\n",
    "            use_skip_connections=use_skip_connections,use_batch_norm=True)(i)\n",
    "else:\n",
    "    m = TCN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,dropout_rate=dropout_rate,\n",
    "            use_skip_connections=use_skip_connections)(i)  \n",
    "\n",
    "m = tf.reshape(m, [-1, nb_filters, 1])\n",
    "m = MaxPooling1D(pool_size=(2))(m)\n",
    "m = Flatten()(m)\n",
    "m = Dense(32, activation='linear', name='pre')(m)\n",
    "output1 = Dense(1, activation='linear', name='velx')(m)\n",
    "output2 = Dense(1, activation='linear', name='vely')(m)\n",
    "model = Model(inputs=[i], outputs=[output1, output2])\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(loss={'velx': 'mse','vely':'mse'},optimizer=opt)  \n",
    "checkpoint = ModelCheckpoint(model_name, monitor='loss', verbose=1, save_best_only=True)\n",
    "model.fit(x=X, y=[x_vel, y_vel],epochs=model_epochs, shuffle=True,callbacks=[checkpoint],batch_size=batch_size)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902af150",
   "metadata": {},
   "source": [
    "## Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cee659",
   "metadata": {},
   "source": [
    "#### Velocity Prediction RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name,custom_objects={'TCN': TCN})\n",
    "y_pred = model.predict(X_test)\n",
    "rmse_vel_x = mean_squared_error(x_vel_test, y_pred[0], squared=False)\n",
    "rmse_vel_y = mean_squared_error(y_vel_test, y_pred[1], squared=False)\n",
    "print('Vel_X RMSE, Vel_Y RMSE:',rmse_vel_x,rmse_vel_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e8e5c",
   "metadata": {},
   "source": [
    "#### ATE and RTE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = size_of_each_test[0]\n",
    "ATE = []\n",
    "RTE = []\n",
    "ATE_dist = []\n",
    "RTE_dist = []\n",
    "for i in range(len(size_of_each_test)):\n",
    "    X_test_sel = X_test[a:b,:,:]\n",
    "    x_vel_test_sel = x_vel_test[a:b]\n",
    "    y_vel_test_sel = y_vel_test[a:b]\n",
    "    Y_head_test_sel = Y_head_test[a:b]\n",
    "    Y_disp_test_sel = Y_disp_test[a:b]\n",
    "    if(i!=len(size_of_each_test)-1):\n",
    "        a += size_of_each_test[i]\n",
    "        b += size_of_each_test[i]\n",
    "\n",
    "    y_pred = model.predict(X_test_sel)\n",
    "\n",
    "    pointx = []\n",
    "    pointy = []\n",
    "    Lx =  x0_list_test[i]\n",
    "    Ly = y0_list_test[i]\n",
    "    for j in range(len(x_vel_test_sel)):\n",
    "        Lx = Lx + (x_vel_test_sel[j]/(((window_size-stride)/stride)))\n",
    "        Ly = Ly + (y_vel_test_sel[j]/(((window_size-stride)/stride)))    \n",
    "        pointx.append(Lx)\n",
    "        pointy.append(Ly)   \n",
    "    Gvx = pointx\n",
    "    Gvy = pointy\n",
    "    \n",
    "    pointx = []\n",
    "    pointy = []\n",
    "    Lx =  x0_list_test[i]\n",
    "    Ly = y0_list_test[i]\n",
    "    for j in range(len(x_vel_test_sel)):\n",
    "        Lx = Lx + (y_pred[0][j]/(((window_size-stride)/stride)))\n",
    "        Ly = Ly + (y_pred[1][j]/(((window_size-stride)/stride)))\n",
    "        pointx.append(Lx)\n",
    "        pointy.append(Ly)\n",
    "    Pvx = pointx\n",
    "    Pvy = pointy    \n",
    "    \n",
    "    at, rt, at_all, rt_all = Cal_TE(Gvx, Gvy, Pvx, Pvy,\n",
    "                                    sampling_rate=sampling_rate,window_size=window_size,stride=stride)\n",
    "    ATE.append(at)\n",
    "    RTE.append(rt)\n",
    "    ATE_dist.append(Cal_len_meters(Gvx, Gvy))\n",
    "    RTE_dist.append(Cal_len_meters(Gvx, Gvy, 600))\n",
    "    print('ATE, RTE, Trajectory Length, Trajectory Length (60 seconds)',ATE[i],RTE[i],ATE_dist[i],RTE_dist[i])\n",
    "    \n",
    "print('Median ATE and RTE', np.median(ATE),np.median(RTE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572d366",
   "metadata": {},
   "source": [
    "#### Sample Trajectory Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can use the size_of_each_test variable to control the region to plot. We plot for the last trajectory\n",
    "a = sum(size_of_each_test[0:5])\n",
    "b = sum(size_of_each_test[0:5])+900\n",
    "\n",
    "X_test_sel = X_test[a:b,:,:]\n",
    "x_vel_test_sel = x_vel_test[a:b]\n",
    "y_vel_test_sel = y_vel_test[a:b]\n",
    "Y_head_test_sel = Y_head_test[a:b]\n",
    "Y_disp_test_sel = Y_disp_test[a:b]\n",
    "\n",
    "y_pred = model.predict(X_test_sel)\n",
    "\n",
    "pointx = []\n",
    "pointy = []\n",
    "Lx =  x0_list_test[i]\n",
    "Ly = y0_list_test[i]\n",
    "for j in range(len(x_vel_test_sel)):\n",
    "    Lx = Lx + (x_vel_test_sel[j]/(((window_size-stride)/stride)))\n",
    "    Ly = Ly + (y_vel_test_sel[j]/(((window_size-stride)/stride)))    \n",
    "    pointx.append(Lx)\n",
    "    pointy.append(Ly)   \n",
    "Gvx = pointx\n",
    "Gvy = pointy\n",
    "\n",
    "pointx = []\n",
    "pointy = []\n",
    "Lx =  x0_list_test[i]\n",
    "Ly = y0_list_test[i]\n",
    "for j in range(len(x_vel_test_sel)):\n",
    "    Lx = Lx + (y_pred[0][j]/(((window_size-stride)/stride)))\n",
    "    Ly = Ly + (y_pred[1][j]/(((window_size-stride)/stride)))\n",
    "    pointx.append(Lx)\n",
    "    pointy.append(Ly)\n",
    "Pvx = pointx\n",
    "Pvy = pointy  \n",
    "\n",
    "print('Plotting Trajectory of length (meters): ',Cal_len_meters(Gvx, Gvy))\n",
    "\n",
    "ptox = Pvx\n",
    "ptoy = Pvy\n",
    "\n",
    "plt.plot(Gvx,Gvy,label='Ground Truth',color='salmon')\n",
    "plt.plot(ptox,ptoy,label='TinyOdom',color='green',linestyle='-')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.title('PDR - OxIOD Dataset')\n",
    "plt.xlabel('East (m)')\n",
    "plt.ylabel('North (m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e21180",
   "metadata": {},
   "source": [
    "#### Error Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2390313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the last trajectory\n",
    "\n",
    "a = sum(size_of_each_test[0:5])\n",
    "b = sum(size_of_each_test[0:5])+size_of_each_test[5]\n",
    "\n",
    "X_test_sel = X_test[a:b,:,:]\n",
    "x_vel_test_sel = x_vel_test[a:b]\n",
    "y_vel_test_sel = y_vel_test[a:b]\n",
    "Y_head_test_sel = Y_head_test[a:b]\n",
    "Y_disp_test_sel = Y_disp_test[a:b]\n",
    "\n",
    "y_pred = model.predict(X_test_sel)\n",
    "\n",
    "pointx = []\n",
    "pointy = []\n",
    "Lx =  x0_list_test[i]\n",
    "Ly = y0_list_test[i]\n",
    "for j in range(len(x_vel_test_sel)):\n",
    "    Lx = Lx + (x_vel_test_sel[j]/(((window_size-stride)/stride)))\n",
    "    Ly = Ly + (y_vel_test_sel[j]/(((window_size-stride)/stride)))    \n",
    "    pointx.append(Lx)\n",
    "    pointy.append(Ly)   \n",
    "Gvx = pointx\n",
    "Gvy = pointy\n",
    "\n",
    "pointx = []\n",
    "pointy = []\n",
    "Lx =  x0_list_test[i]\n",
    "Ly = y0_list_test[i]\n",
    "for j in range(len(x_vel_test_sel)):\n",
    "    Lx = Lx + (y_pred[0][j]/(((window_size-stride)/stride)))\n",
    "    Ly = Ly + (y_pred[1][j]/(((window_size-stride)/stride)))\n",
    "    pointx.append(Lx)\n",
    "    pointy.append(Ly)\n",
    "Pvx = pointx\n",
    "Pvy = pointy  \n",
    "\n",
    "at, rt, at_all, rt_all = Cal_TE(Gvx, Gvy, Pvx, Pvy,\n",
    "                                    sampling_rate=sampling_rate,window_size=window_size,stride=stride)\n",
    "\n",
    "x_ax = np.linspace(0,60,len(rt_all))\n",
    "print('Plotting for trajectory of length (meters): ',Cal_len_meters(Gvx, Gvy))\n",
    "\n",
    "plt.plot(x_ax,rt_all,label='TinyOdom',color='green',linestyle='-')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Position Error (m)')\n",
    "plt.title('PDR - OxIOD Dataset')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7104c51",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d766d",
   "metadata": {},
   "source": [
    "#### Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tflite_model(model=model,training_data=X_tr,quantization=quantization,output_name='g_model.tflite') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75ff75",
   "metadata": {},
   "source": [
    "#### Conversion to C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_cpp_model(dirpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
