{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2e083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess2 = tf.compat.v1.Session(config=config)\n",
    "set_session(sess2)  \n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tcn import compiled_tcn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from keras_flops import get_flops\n",
    "from mango.tuner import Tuner\n",
    "import time\n",
    "import pickle\n",
    "from hardware_utils import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b2bf5",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100\n",
    "window_size = 550\n",
    "stride = 50\n",
    "f = '/home/nesl/earable/Earable/Activity_Dataset/' #dataset directory\n",
    "\n",
    "X_tr, Y_tr, X_test, Y_test = import_auritus_activity_dataset(dataset_folder = f, \n",
    "                                use_timestamp=False, \n",
    "                                shuffle=True, \n",
    "                                window_size = window_size, stride = stride, \n",
    "                                return_test_set = True, test_set_size = 300,channels=0)\n",
    "\n",
    "print(X_tr.shape)\n",
    "print(Y_tr.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913b0e7",
   "metadata": {},
   "source": [
    "# Training and NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f849021",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath=\"/home/nesl/Mbed Programs/auritus_tcn/\" #hardware program directory - this is where the TCN deployment code is stored\n",
    "device = \"NUCLEO_F446RE\" #which hardware to use\n",
    "model_name = 'Auritus_HIL'+device+'.hdf5'\n",
    "HIL = True #use real hardware or proxy?\n",
    "quantization = False #use quantization or not?\n",
    "model_epochs = 900 #epochs to train each model for\n",
    "NAS_epochs = 50 #epochs for hyperparameter tuning\n",
    "output_name = 'g_model.tflite'\n",
    "log_file_name = 'TCN_Auritus_'+device+'.csv'\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "row_write = ['score', '1-accuracy','RAM','Flash','FLOPS','Latency','nb_filters','kernel_size',\n",
    "             'dilations','use_skip_connections']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(epochs=1000,nb_filters=10,kernel_size=3,\n",
    "                 dilations=[1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "                 use_skip_connections=True):\n",
    "    \n",
    "    err = 'inf'\n",
    "    input_dim=X_tr.shape[2]\n",
    "    \n",
    "    model = compiled_tcn(return_sequences=False,\n",
    "                         num_feat=input_dim,\n",
    "                         num_classes=Y_tr.shape[1],\n",
    "                         nb_filters=nb_filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         dilations=dilations,\n",
    "                         nb_stacks=1,\n",
    "                         max_len=window_size,\n",
    "                         use_weight_norm=False,\n",
    "                         use_skip_connections=use_skip_connections)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=opt,metrics=['accuracy'])\n",
    "    Flops = get_flops(model, batch_size=1)\n",
    "    convert_to_tflite_model(model=model,training_data=X_tr,quantization=quantization,output_name=output_name) \n",
    "    maxRAM, maxFlash = return_hardware_specs(device)\n",
    "    \n",
    "    if(HIL==True):\n",
    "        convert_to_cpp_model(dirpath)\n",
    "        RAM, Flash, Latency, idealArenaSize, errorCode = HIL_controller(dirpath=dirpath,\n",
    "                                                                       chosen_device=device,\n",
    "                                                                       window_size=window_size, \n",
    "                                                                    number_of_channels = input_dim,\n",
    "                                                                   quantization=quantization)     \n",
    "        score = -5.0\n",
    "        if(Flash==-1):\n",
    "            row_write = [score, err,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,use_skip_connections]\n",
    "            print('Design choice:',row_write)\n",
    "            with open(log_file_name, 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(row_write)\n",
    "            return score    \n",
    "    \n",
    "        elif(Flash!=-1):\n",
    "            checkpoint = ModelCheckpoint(model_name, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "            model.fit(x=X_tr, y=Y_tr,validation_split=0.1,\n",
    "                      epochs=epochs,callbacks=[checkpoint],shuffle=True,verbose=1)\n",
    "            err = 1-checkpoint.best\n",
    "            resource_usage = (RAM/maxRAM) + (Flash/maxFlash) \n",
    "            score = -err + 0.01*resource_usage - 0.05*Latency #weigh each component as you like\n",
    "                \n",
    "            row_write = [score, err,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,use_skip_connections]\n",
    "            print('Design choice:',row_write)\n",
    "            with open(log_file_name, 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(row_write)   \n",
    "    else:\n",
    "        score = -5.0\n",
    "        Flash = os.path.getsize(output_name)\n",
    "        RAM = get_model_memory_usage(batch_size=1,model=model)\n",
    "        Latency=-1\n",
    "        max_flops = (30e6)\n",
    "        \n",
    "        if(RAM < maxRAM and Flash<maxFlash):\n",
    "            checkpoint = ModelCheckpoint(model_name, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "            model.fit(x=X_tr, y=Y_tr,validation_split=0.1,\n",
    "                      epochs=epochs,callbacks=[checkpoint],shuffle=True,verbose=1)\n",
    "            err = 1-checkpoint.best \n",
    "            resource_usage = (RAM/maxRAM) + (Flash/maxFlash)\n",
    "            score = -err + 0.01*resource_usage - 0.05*(Flops/max_flops) #weigh each component as you like\n",
    "        \n",
    "        row_write = [score, err,RAM,Flash,Flops,Latency,\n",
    "                 nb_filters,kernel_size,dilations,use_skip_connections]\n",
    "        print('Design choice:',row_write)\n",
    "        with open(log_file_name, 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(row_write) \n",
    "    \n",
    "    return score        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_res(data, file_name):\n",
    "    pickle.dump( data, open( file_name, \"wb\" ) )\n",
    "    \n",
    "min_layer = 3\n",
    "max_layer = 8\n",
    "a_list = [1,2,4,8,16,32,64,128,256]\n",
    "all_combinations = []\n",
    "dil_list = []\n",
    "for r in range(len(a_list) + 1):\n",
    "    combinations_object = itertools.combinations(a_list, r)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list\n",
    "all_combinations = all_combinations[1:]\n",
    "for item in all_combinations:\n",
    "    if(len(item) >= min_layer and len(item) <= max_layer):\n",
    "        dil_list.append(list(item))\n",
    "\n",
    "param_dict = {\n",
    "    'nb_filters': range(2,64),\n",
    "    'kernel_size': range(2,16),\n",
    "    'use_skip_connections': [True, False],\n",
    "    'dil_list': dil_list\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        nb_filters = hyper_par['nb_filters']\n",
    "        kernel_size = hyper_par['kernel_size']\n",
    "        use_skip_connections = hyper_par['use_skip_connections']\n",
    "        dil_list = hyper_par['dil_list']\n",
    "            \n",
    "        objective = objective_NN(epochs=model_epochs,nb_filters=nb_filters,kernel_size=kernel_size,\n",
    "                                 dilations=dil_list,use_skip_connections=use_skip_connections,)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated\n",
    "\n",
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = NAS_epochs\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b09dd",
   "metadata": {},
   "source": [
    "# Train the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07208a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=X_tr.shape[2]\n",
    "\n",
    "model = compiled_tcn(return_sequences=False,\n",
    "                     num_feat=input_dim,\n",
    "                     num_classes=Y_tr.shape[1],\n",
    "                     nb_filters=results['best_params']['nb_filters'],\n",
    "                     kernel_size=results['best_params']['kernel_size'],\n",
    "                     dilations=results['best_params']['dilations'],\n",
    "                     nb_stacks=1,\n",
    "                     max_len=window_size,\n",
    "                     use_weight_norm=False,\n",
    "                     use_skip_connections=results['best_params']['use_skip_connections'])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "model.fit(x=X_tr, y=Y_tr,validation_split=0.1,\n",
    "          epochs=model_epochs,callbacks=[checkpoint],shuffle=True,verbose=1)\n",
    "\n",
    "model = load_model(model_name,custom_objects={'TCN': TCN})\n",
    "test_accu = model.evaluate(x=X_test,y=Y_test)[1]\n",
    "print('Test Accuracy:', test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c771e76",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee078ce",
   "metadata": {},
   "source": [
    "### Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tflite_model(model=model,training_data=X_tr,quantization=quantization,output_name=output_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02964c4",
   "metadata": {},
   "source": [
    "### Conversion to C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_cpp_model(dirpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
